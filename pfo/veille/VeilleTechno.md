# Agip SHAFIK                   
*Date d'editon:27/12/2023





![veille technologique](VeilleTechno/veille-technologique.jpg)



## processeur


09/12/2024

NVIDIA ouvre son premier centre de R&D au Vietnam, soulignant son engagement envers le développement de l’IA dans le pays. Ce centre se concentrera sur le développement logiciel, en collaborant avec le gouvernement, les startups, les universités et les entreprises pour accélérer 
l’adoption de l’IA. Il soutiendra des secteurs clés comme la santé, l’éducation, les transports et la finance. NVIDIA, actif au Vietnam depuis huit ans, renforce ainsi sa contribution à l’écosystème technologique et à la stratégie de transformation numérique du pays.
https://nvidianews.nvidia.com/news/nvidia-to-open-vietnam-r-d-center-to-bolster-ai-development


03/12/2024

L'intégration de NVIDIA CUDA-Q avec Amazon Braket révolutionne l'informatique quantique en combinant les capacités des unités de traitement quantique (QPU) avec des superordinateurs IA. Ce paradigme, appelé supercalcul quantique accéléré, facilite la correction d'erreurs quantiques, l'optimisation du contrôle matériel, et l'exécution d'algorithmes hybrides. CUDA-Q exploite la puissance des GPU pour accélérer les tâches classiques liées au quantique, tandis qu'Amazon Braket offre un accès à des plateformes quantiques variées via le cloud. Cette collaboration permet une meilleure intégration entre calcul quantique et IA.
https://developer.nvidia.com/blog/accelerated-quantum-supercomputing-with-the-nvidia-cuda-q-and-amazon-braket-integration/
31/10/2024


Intel a récemment lancé le Gaudi 3, un nouvel accélérateur d'IA destiné aux centres de données, avec une bande-annonce dramatique qui rappelle les superproductions hollywoodiennes, notamment grâce à une bande sonore puissante et un design visuel impressionnant. Gaudi 3 vise à concurrencer Nvidia et AMD dans le domaine des accélérateurs IA, bien qu'avec une approche centrée sur l'efficacité énergétique et le rapport performance/prix. Il promet une augmentation de 70 % de la vitesse d'entraînement et 50 % de la vitesse d'inférence par rapport au modèle H100 de Nvidia, tout en étant environ deux fois plus rentable pour certains usages d'inférence.

Conçu pour s'adapter aux besoins variés de l'IA, Gaudi 3 supporte les formats FP8 et BF16, essentiels pour des calculs intensifs tout en maintenant une efficacité énergétique. Par exemple, à environ 600 W, il offre jusqu'à 1 835 TFLOPS en BF16, ce qui le rend concurrentiel face aux unités H100 de Nvidia dans certains scénarios spécifiques. Il sera notamment accessible via le cloud d’IBM et d'autres plateformes partenaires, offrant aux développeurs des options diversifiées pour les charges de travail d'IA.

Si cela vous intéresse, vous pouvez découvrir la bande-annonce complète et d'autres informations 

YOUTUBE
https://www.youtube.com/watch?v=Rjjkvgcu0H8).




07/10/2024 AI Supercomputer



**NVIDIA GB200 NVL72**


Nvidia et FOXCON on realiser un ordinateurs superpuiss ou y a l'ia integrer 




https://blogs.nvidia.com/blog/foxconn-taiwan-blackwell/

08/10/2024 NVIDIA pourrait enfin résoudre son problème de VRAM 

NVIDIA prépare des innovations pour sa gamme RTX 5000, avec notamment la RTX 5090 pour ordinateurs portables qui pourrait offrir 24 Go de VRAM grâce à des modules GDDR7. Toutefois, son lancement serait retardé, probablement jusqu'en 2025, en raison de la disponibilité limitée de ces nouveaux modules de mémoire. Les modèles RTX 5080, 5070, et 5060 pourraient être dévoilés plus tôt avec des capacités de mémoire plus modestes. Des annonces concrètes sont attendues lors du CES 2025, où NVIDIA devrait révéler plus d'informations sur ces nouvelles cartes.

https://www.omgpu.com/nvidia-pourrait-enfin-resoudre-son-probleme-de-vram-mais-cela-prendra-du-temps/


17/09/2024 GPU sans IA ?
NVIDIA, avec sa génération de GPU Turing, a bouleversé l'industrie du jeu vidéo en intégrant des unités spécifiques dédiées à l'intelligence artificielle (IA). Cette approche a transformé la manière de développer des jeux, notamment à travers le DLSS, une technologie d'upscaling d'abord critiquée, mais qui a permis de contourner les contraintes de performances sans nécessiter des GPU plus puissants et coûteux. Le DLSS a progressivement évolué pour offrir une meilleure qualité d'image et réduire les flous liés aux mouvements, avec des versions comme le DLSS 3.5 qui améliorent la fidélité des effets de ray tracing même sur des cartes plus anciennes. Par ailleurs, NVIDIA développe d'autres technologies basées sur l'IA pour rendre les jeux plus immersifs, améliorer les textures et donner aux personnages non jouables des comportements plus "humains". La compression de textures neuronales est un autre exemple de l'utilisation de l'IA pour optimiser les ressources tout en améliorant les graphismes. Face à cette avancée, les concurrents comme AMD et Intel s'efforcent de suivre le rythme, bien que des écarts de performance subsistent, notamment avec le FSR d'AMD qui, malgré sa prochaine version 4 intégrant l'IA, reste en retrait par rapport au DLSS. Cette course technologique soulève néanmoins des préoccupations quant à une éventuelle fracture entre NVIDIA et le reste de l'industrie, mais il semble que l'IA s'impose comme un incontournable pour tous les fabricants de GPU.







https://hardwareand.co/actualites/breves/gpu-sans-ia-plus-jamais-chez-nvidia-en-tout-cas

30/05/2023 Le monde est au "point de bascule d’une nouvelle ère informatique",

Jensen Huang, PDG de Nvidia, a prononcé son premier discours public en quatre ans lors du salon Computex à Taïwan. Il a souligné que l'IA était à un "point de bascule", présentant une plateforme de superordinateur d'IA appelée DGX GH200, en pleine production. Google Cloud, Meta et Microsoft sont parmi les premières entreprises à y avoir accès. Huang a noté que cette plateforme facilitera la création de produits liés à l'IA en permettant des tâches informatiques plus complexes. La valeur boursière de Nvidia a augmenté de 25 % en une nuit après des résultats trimestriels très positifs, portés par la demande de puces électroniques pour l'IA. Fondée il y a 30 ans par Huang, Nvidia est connue pour ses puces graphiques utilisées dans les jeux vidéo, devenues des moteurs de l'IA.

https://www.lesnumeriques.com/cpu-processeur/computex-2023-le-monde-est-au-point-de-bascule-d-une-nouvelle-ere-informatique-selon-le-patron-de-nvidia-n210106.html

TSMC annonce que ses premières tentatives de production en 7 nm prendront place au cours de la première moitié de l’année 2017. Dès aujourd’hui, le fondeur explique avoir déjà 20 clients engagés dans cette production, espérant avoir 15 puces finalisées en 7 nm en 2017. TSMC a déjà lancé la production en 10 nm au dernier trimestre.

https://auzous.wordpress.com/2016/04/21/premiers-processeurs-graves-en-7-nm-chez-tsmc-debut-2017/





Intel a présenté Meteor Lake, sa nouvelle architecture de processeur, lors de l'événement InnovatiON 2023. Il s'agit de la première architecture "disaggregated design" d'Intel, utilisant des chiplets avec la technologie 3D Foveros. Meteor Lake intègre divers modules, tels que Compute Tiles, SOC Tiles, et IO Tiles, interconnectés pour améliorer la flexibilité de conception.

Intel introduit également de nouveaux éléments, dont des Low Power Island E-cores dans le SOC Tile et un NPU AI Engine dédié à l'IA. Le processeur utilise le nouveau processus de fabrication Intel 4. Les premiers processeurs Meteor Lake seront disponibles le 14 décembre, initialement pour les laptops, avec une gamme de 5 à 14 Cores et des enveloppes thermiques de 7 à 45 watts.





https://www.cowcotland.com/news/88334/intel-annonce-sa-nouvelle-architecture-meteor-lake-qui-arrivera-le-14-decembre-prochain.html



Keysom, une start-up créée en juillet 2022, cherche à démocratiser la conception de processeurs avec un logiciel "no code". Leur solution permet aux utilisateurs sans compétences en programmation ou en hardware de créer des architectures de processeurs en faisant simplement glisser-déposer du code. La société, basée à Pessac, a remporté le Prix du Public lors des 16e Assises de l'Embarqué en janvier 2024.

Le logiciel de Keysom utilise une exploration architecturale basée sur des algorithmes génératifs, comblant ainsi une lacune dans les logiciels de conception électronique. Les clients, issus des domaines de l'automobile, de l'IoT, de l'IA et de la robotique, créent des systèmes sur puce. Keysom se concentre sur l'architecture Risc-V en 32 bits, un standard industriel sans licence.

Les fondateurs, Luca Testa et Cyril Sagonero, ont observé une utilisation de processeurs surdimensionnés dans leurs projets antérieurs, motivant la création de Keysom. La start-up espère obtenir un financement de trois millions d'euros pour poursuivre son développement et collaborer avec des centres de recherche français sur de nouvelles architectures adaptées à l'IA et aux véhicules autonomes.

https://www.usinenouvelle.com/article/comment-keysom-utilise-des-algorithmes-generatifs-pour-democratiser-la-conception-de-processeurs.N2206578



22/01/2024
Nvidia aurait signé un accord avec TSMC pour la production de sa prochaine génération de cartes graphiques gravées en 3 nm, nom de code "Blackwell". Ces GPU, comme le GB100 destiné aux serveurs, utiliseraient une conception en chiplet pour améliorer les performances par watt. Le passage au 3 nm est crucial pour maintenir la suprématie technique de Nvidia dans le domaine de l'IA. Si ces informations sont correctes, Nvidia pourrait lancer ses nouveaux GPU à la fin de 2024, donnant à la société un avantage sur ses concurrents, notamment AMD et Intel, qui ne devraient pas passer au 3 nm avant 2025 ou 2026.


https://www.frandroid.com/marques/nvidia/1812387_nvidia-blackwell-ces-prochaines-cartes-graphiques-seraient-gravees-en-3-nm-des-2024 



26 septembre 2023 


Lors de l'événement "Scary Fast", Apple a présenté sa nouvelle gamme de processeurs M3, M3 Pro et M3 Max gravés en 3 nm. Ces processeurs offrent des améliorations significatives, notamment une augmentation de 65 % des performances GPU pour les tâches gourmandes. Les processeurs intègrent également un CPU et un Neural Engine plus rapides, ainsi qu'une mémoire unifiée allant jusqu'à 128 Go. Grâce à une gravure en 3 nm, ces processeurs offrent des performances graphiques avancées, avec des vitesses de rendu jusqu'à 2,5 fois plus rapides que la génération précédente. Le CPU présente des cœurs de performance jusqu'à 30 % plus rapides et des cœurs à haute efficacité énergétique jusqu'à 50 % plus rapides, offrant des performances multithread équivalentes à la génération précédente tout en consommant moitié moins d'énergie. Le Neural Engine est 60 % plus rapide, et un moteur médias prend en charge le décodage AV1 pour des expériences vidéo de meilleure qualité. Les modèles M3, M3 Pro et M3 Max offrent différentes configurations en termes de cœurs CPU et GPU, ainsi que des options de mémoire allant jusqu'à 128 Go.

https://www.lemondeinformatique.fr/actualites/lire-tsmc-presente-aux-clients-dont-apple-la-fabrication-de-puces-en-2-nm-92453.html


09/04/2023

https://www.actuia.com/actualite/google-cloud-et-nvidia-un-partenariat-etendu-autour-des-donnees-et-de-lia-generative/

Google Cloud et NVIDIA ont annoncé une collaboration pour accélérer le développement de l'IA générative. NVIDIA DGX Cloud sera bientôt disponible sur Google Cloud, offrant aux entreprises un accès à une infrastructure avancée avec des GPU H100 pour l'entraînement des modèles IA. PaxML, le framework IA de Google, est optimisé pour ces GPU, et les machines virtuelles A3 de Google Cloud, également alimentées par des H100, sont maintenant disponibles pour accélérer les modèles de langage massif. Google Cloud utilisera aussi le supercalculateur DGX GH200 AI de NVIDIA pour ses projets d'IA.
